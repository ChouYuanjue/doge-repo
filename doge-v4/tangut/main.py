import os
import json
import pickle
from PIL import Image, ImageDraw, ImageFont
from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, register
from astrbot.api import logger
from collections import defaultdict
import jieba
import re

# æœ¬æ’ä»¶è°ƒç”¨çš„è¥¿å¤æ–‡è¯å…¸å’Œå­—å…¸æ•°æ®æ¥è‡ªäºå¤ä»Šæ–‡å­—é›†æˆ(ccamc.co)ï¼Œç”±Githubç”¨æˆ·tinbreakerçˆ¬å–ï¼Œåœ¨æ­¤å¯¹äºŒè€…è¡¨ç¤ºæ„Ÿè°¢

class OptimizedDictEntry:
    """ä½¿ç”¨__slots__ä¼˜åŒ–å†…å­˜çš„å­—å…¸æ¡ç›®ç±»ï¼Œå…¼å®¹ä¸¤ç§æ ¼å¼"""
    __slots__ = ('key', 'GX', 'GHC', 'LFW', 'explanationEN', 'explanationCN', 'entry_type')
    
    def __init__(self, data):
        # ç¡®å®šæ¡ç›®ç±»å‹å’Œä¸»é”®
        if "word" in data:
            self.key = data["word"]
            self.entry_type = "word"
        elif "character" in data:
            self.key = data["character"]
            self.entry_type = "character"
        else:
            self.key = ""
            self.entry_type = "unknown"
        
        self.GX = data.get("GX", "")
        self.GHC = data.get("GHC", "")
        self.LFW = data.get("LFW", "")
        self.explanationEN = data.get("explanationEN", "")
        self.explanationCN = data.get("explanationCN", "")
    
    def to_dict(self):
        """å°†æ¡ç›®è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆç”¨äºåºåˆ—åŒ–ï¼‰"""
        result = {attr: getattr(self, attr) for attr in self.__slots__ if hasattr(self, attr)}
        # æ ¹æ®æ¡ç›®ç±»å‹è®¾ç½®æ­£ç¡®çš„é”®å
        if self.entry_type == "word":
            result["word"] = result.pop("key")
        elif self.entry_type == "character":
            result["character"] = result.pop("key")
        return result
    
    def get_display_key(self):
        """è·å–ç”¨äºæ˜¾ç¤ºçš„é”®å"""
        return f"{self.entry_type}: {self.key}"

class BilingualDictionary:
    """åŒå‘æŸ¥è¯¢è¯å…¸ç±»ï¼Œç›´æ¥ä»JSONåŠ è½½"""
    
    def __init__(self, dict_file_path):
        """ç›´æ¥åŠ è½½JSONæ ¼å¼çš„è¯å…¸"""
        print("æ­£åœ¨åŠ è½½è¯å…¸...")
        
        # åˆå§‹åŒ–ç´¢å¼•ç»“æ„
        self.forward_index = {}
        self.reverse_index = defaultdict(list)
        
        # åŠ è½½JSONæ–‡ä»¶å¹¶æ„å»ºç´¢å¼•
        with open(dict_file_path, 'r', encoding='utf-8') as f:
            dict_data = json.load(f)
            self._build_indexes(dict_data)
        
        print("è¯å…¸åŠ è½½å®Œæˆ!")
        
        # åˆå§‹åŒ–å­—ç¬¦ç»„åˆè§„åˆ™
        self.combine_rules = self._initialize_combine_rules()
    
    def _build_indexes(self, dict_data):
        """ä»JSONæ•°æ®æ„å»ºæ­£å‘å’Œåå‘ç´¢å¼•"""
        for item in dict_data:
            entry = OptimizedDictEntry(item)
            
            # æ·»åŠ åˆ°æ­£å‘ç´¢å¼•ï¼ˆè¥¿å¤æ–‡ -> æ¡ç›®ï¼‰
            self.forward_index[entry.key] = entry
            
            # æ·»åŠ åˆ°åå‘ç´¢å¼•ï¼ˆå…³é”®è¯ -> è¥¿å¤æ–‡ï¼‰
            # ä»ä¸­æ–‡è§£é‡Šä¸­æå–å…³é”®è¯
            if entry.explanationCN:
                # ç®€å•åˆ†è¯
                cn_keywords = [kw.strip() for kw in re.findall(r'[^ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]+', entry.explanationCN) if kw.strip()]
                for kw in cn_keywords:
                    # æ’é™¤æ•°å­—å’Œç‰¹æ®Šæ ‡è®°
                    if not re.match(r'^[0-9ã€ã€‘]+$', kw):
                        self.reverse_index[kw].append(entry.key)
            
            # ä»è‹±æ–‡è§£é‡Šä¸­æå–å…³é”®è¯
            if entry.explanationEN:
                en_keywords = [kw.strip().lower() for kw in re.split(r'[\s.,;]+', entry.explanationEN) if kw.strip()]
                for kw in en_keywords:
                    self.reverse_index[kw].append(entry.key)
    
    def _initialize_combine_rules(self):
        """åˆå§‹åŒ–å­—ç¬¦ç»„åˆè§„åˆ™ç³»ç»Ÿ"""
        # åŸºç¡€ç»„åˆæ¨¡æ¿
        combine_templates = {
            'PREV_EQUAL': {'combineWithPrevious': True, 'connector': '='},
            'PREV_HYPHEN': {'combineWithPrevious': True, 'connector': '-'},
            'NEXT_HYPHEN': {'combineWithNext': True, 'connector': '-'}
        }
        
        # éœ€è¦ç‰¹æ®Šå¤„ç†ç»„åˆçš„å­—ç¬¦
        specific_rules = {
            'ğ—§“': {
                'variants': [
                    {'type': 'standalone', 'condition': lambda prev, next: not self._is_valid_char(prev), 
                     'GX': 'S1', 'GHC': 'Pronoun'}, 
                    {'type': 'combineWithPrevious', 'connector': '-', 
                     'condition': lambda prev, next: self._is_valid_char(prev), 
                     'GX': 'S2', 'GHC': 'Affix'}
                ]
            },
            
        }
        
        prev_equal_chars = ['ğ—«‚', 'ğ—…', 'ğ˜†„', 'ğ—‡‹', 'ğ——™', 'ğ—¦‡']
        for char in prev_equal_chars:
            specific_rules[char] = combine_templates['PREV_EQUAL']
        
        prev_hyphen_chars = ['ğ˜‰', 'ğ—±', 'ğ——Ÿ', 'ğ—«¶']
        for char in prev_hyphen_chars:
            specific_rules[char] = combine_templates['PREV_HYPHEN']
        
        return specific_rules
    
    def _is_valid_char(self, char):
        """æ£€æŸ¥å­—ç¬¦æ˜¯å¦ä¸ºæœ‰æ•ˆè¥¿å¤æ–‡å­—ç¬¦ï¼ˆéç¬¦å·ï¼‰"""
        return char and not re.match(r'[\p{P}\s]', char, re.UNICODE)
    
    def search_by_key(self, key):
        """é€šè¿‡è¥¿å¤æ–‡å­—ç¬¦æˆ–è¯æŸ¥è¯¢ç›¸å…³æ¡ç›®ï¼Œæ”¯æŒå­—ç¬¦ç»„åˆå’Œå˜ä½“è§„åˆ™"""
        # 1. å°è¯•ç›´æ¥åŒ¹é…
        if key in self.forward_index:
            return self._apply_variant_rules(key, None, None)
        
        # 2. å°è¯•æ‹†åˆ†ç»„åˆå­—ç¬¦è¿›è¡ŒåŒ¹é…
        combined_results = self._process_character_combinations(key)
        if combined_results:
            return combined_results
        
        # 3. å°è¯•æ¨¡ç³ŠåŒ¹é…
        return self.fuzzy_search_key(key)
        
    def _process_character_combinations(self, input_str):
        chars = list(input_str)
        results = []
        i = 0
        
        while i < len(chars):
            current_char = chars[i]
            
            # æ£€æŸ¥å½“å‰å­—ç¬¦æ˜¯å¦æœ‰ç»„åˆè§„åˆ™
            if current_char in self.combine_rules:
                rule = self.combine_rules[current_char]
                
                # å¤„ç†å‘å‰ç»„åˆ
                if rule.get('combineWithPrevious') and i > 0:
                    combined_char = f"{chars[i-1]}{rule['connector']}{current_char}"
                    if combined_char in self.forward_index:
                        results.append(self._apply_variant_rules(combined_char, chars[i-2] if i > 1 else None, chars[i+1] if i < len(chars)-1 else None))
                        i += 1  # è·³è¿‡ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œå› ä¸ºå·²ç»„åˆ
                        continue
                
                # å¤„ç†å‘åç»„åˆ
                if rule.get('combineWithNext') and i < len(chars)-1:
                    combined_char = f"{current_char}{rule['connector']}{chars[i+1]}"
                    if combined_char in self.forward_index:
                        results.append(self._apply_variant_rules(combined_char, chars[i-1] if i > 0 else None, chars[i+2] if i < len(chars)-2 else None))
                        i += 2  # è·³è¿‡å·²ç»„åˆçš„ä¸‹ä¸€ä¸ªå­—ç¬¦
                        continue
            
            # å¦‚æœæ²¡æœ‰ç»„åˆè§„åˆ™ï¼Œç›´æ¥æŸ¥è¯¢å•ä¸ªå­—ç¬¦
            if current_char in self.forward_index:
                results.append(self._apply_variant_rules(current_char, chars[i-1] if i > 0 else None, chars[i+1] if i < len(chars)-1 else None))
            
            i += 1
        
        return results if results else None
        
    def _apply_variant_rules(self, key, prev_char, next_char):
        """åº”ç”¨å˜ä½“è§„åˆ™ï¼Œå‚è€ƒapp.jsçš„getExplanationé€»è¾‘"""
        entry = self.forward_index.get(key)
        if not entry or key not in self.combine_rules:
            return entry
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å˜ä½“è§„åˆ™
        rule = self.combine_rules[key]
        if 'variants' in rule:
            for variant in rule['variants']:
                if variant['condition'](prev_char, next_char):
                    # åº”ç”¨å˜ä½“å±æ€§
                    for prop in ['GX', 'GHC', 'explanationCN', 'explanationEN']:
                        if prop in variant:
                            setattr(entry, prop, variant[prop])
                    return entry
        
        return entry
    
    def search_by_text(self, text, lang="cn"):
        """
        é€šè¿‡æ–‡æœ¬æŸ¥è¯¢ç›¸å…³è¥¿å¤æ–‡æ¡ç›®
        lang: "cn" ä¸­æ–‡æŸ¥è¯¢, "en" è‹±æ–‡æŸ¥è¯¢
        """
        results = []
        terms = []
        
        if lang == "cn":
            # ä¸­æ–‡æŸ¥è¯¢ï¼Œä½¿ç”¨åˆ†è¯
            terms = [term.strip() for term in jieba.cut(text) if term.strip()]
        else:
            # è‹±æ–‡æŸ¥è¯¢ï¼Œç®€å•åˆ†å‰²
            terms = [term.strip() for term in re.split(r'[\s.,;]+', text) if term.strip()]
        
        for term in terms:
            result_words = self.reverse_index.get(term, [])
            for word in result_words:
                entry = self.forward_index[word]
                if entry not in results:
                    results.append(entry)
        
        # æ ¹æ®æŸ¥è¯¢è¯åœ¨è§£é‡Šä¸­çš„å‡ºç°æ¬¡æ•°æ’åºç»“æœ
        scored_entries = []
        for entry in results:
            if lang == "cn":
                score = sum(entry.explanationCN.count(term) for term in terms)
            else:
                score = sum(entry.explanationEN.count(term) for term in terms)
            scored_entries.append( (entry, score) )
        
        # æŒ‰åˆ†æ•°é™åºæ’åºï¼Œåˆ†æ•°ç›¸åŒåˆ™æŒ‰æ¡ç›®ç±»å‹æ’åº
        sorted_entries = sorted(scored_entries, key=lambda x: (-x[1], x[0].entry_type))
        sorted_results = [entry for entry, score in sorted_entries]
        
        return sorted_results
    
    def search_contains(self, keyword, field="all"):
        """
        æŸ¥æ‰¾æŒ‡å®šå­—æ®µä¸­åŒ…å«å…³é”®è¯çš„æ‰€æœ‰æ¡ç›®
        field: "all", "cn", "en", "gx", "ghc"
        """
        results = []
        keyword = keyword.lower()
        
        for entry in self.forward_index.values():
            match = False
            
            if field in ["all", "cn"] and keyword in entry.explanationCN.lower():
                match = True
            elif field in ["all", "en"] and keyword in entry.explanationEN.lower():
                match = True
            elif field in ["all", "gx"] and keyword in entry.GX.lower():
                match = True
            elif field in ["all", "ghc"] and keyword in entry.GHC.lower():
                match = True
            elif field in ["all", "lfw"] and entry.LFW and keyword in entry.LFW.lower():
                match = True
            
            if match and entry not in results:
                results.append(entry)
        
        return results
    
    def fuzzy_search_key(self, partial_key):
        """æ¨¡ç³ŠæŸ¥è¯¢è¥¿å¤æ–‡ï¼ˆåŒ…å«éƒ¨åˆ†åŒ¹é…ï¼‰"""
        results = []
        for key in self.forward_index.keys():
            if partial_key in key:
                results.append(self.forward_index[key])
        return results
    
    def get_all_keys(self):
        """è·å–æ‰€æœ‰è¥¿å¤æ–‡è¯æ±‡/å­—ç¬¦"""
        return list(self.forward_index.keys())
    
    def get_all_keywords(self):
        """è·å–æ‰€æœ‰ä¸­è‹±æ–‡å…³é”®è¯"""
        return list(self.reverse_index.keys())
    
    def get_stats(self):
        """è·å–è¯å…¸ç»Ÿè®¡ä¿¡æ¯"""
        word_count = 0
        character_count = 0
        unknown_count = 0
        
        for entry in self.forward_index.values():
            if entry.entry_type == "word":
                word_count += 1
            elif entry.entry_type == "character":
                character_count += 1
            else:
                unknown_count += 1
        
        return {
            "total_entries": len(self.forward_index),
            "words": word_count,
            "characters": character_count,
            "unknown_type": unknown_count,
            "keywords": len(self.reverse_index)
        }

# å®‰å…¨åŠ è½½åŒè¯­è¯å…¸çš„å‡½æ•°
def load_bilingual_dictionary(dict_file_path):
    """å®‰å…¨åŠ è½½åŒè¯­è¯å…¸ï¼Œç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç±»åœ¨å‘½åç©ºé—´ä¸­å¯ç”¨"""
    try:
        return BilingualDictionary(dict_file_path)
    except Exception as e:
        logger.error(f"åŠ è½½è¯å…¸å¤±è´¥: {e}")
        raise

# TangutPluginä¸»ç±»
@register("tangut", "runnel", "è¥¿å¤æ–‡æ‹ŸéŸ³å’Œç¿»è¯‘æ’ä»¶", "1.7.0")
class TangutPlugin(Star):
    def __init__(self, context: Context):
        super().__init__(context)
        # è·å–æ’ä»¶ç›®å½•è·¯å¾„
        self.plugin_dir = os.path.dirname(os.path.abspath(__file__))
        # åŠ è½½è¯å…¸
        self.dictionary = None
        self._load_dictionary()
        # å­—ä½“è·¯å¾„
        self.font_path = os.path.join(self.plugin_dir, "NotoSerifTangut-Regular.ttf")
    
    def _load_dictionary(self):
        """åŠ è½½JSONè¯å…¸æ–‡ä»¶"""
        dict_path = os.path.join(self.plugin_dir, "dictionary.json")
        try:
            logger.info(f"å°è¯•åŠ è½½è¯å…¸æ–‡ä»¶: {dict_path}")
            if os.path.exists(dict_path):
                logger.info(f"è¯å…¸æ–‡ä»¶å­˜åœ¨ï¼Œå¤§å°: {os.path.getsize(dict_path)} å­—èŠ‚")
                # ä½¿ç”¨ä¸“é—¨çš„åŠ è½½å‡½æ•°
                self.dictionary = load_bilingual_dictionary(dict_path)
                logger.info("è¥¿å¤æ–‡å­—å…¸åŠ è½½æˆåŠŸ")
                # è¾“å‡ºè¯å…¸ç»Ÿè®¡ä¿¡æ¯
                if self.dictionary:
                    stats = self.dictionary.get_stats()
                    logger.info(f"è¯å…¸ç»Ÿè®¡: {stats}")
            else:
                logger.error(f"è¯å…¸æ–‡ä»¶ä¸å­˜åœ¨: {dict_path}")
        except Exception as e:
            logger.error(f"åŠ è½½è¯å…¸å¤±è´¥: {e}", exc_info=True)
            # å°è¯•æ£€æŸ¥JSONæ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
            try:
                with open(dict_path, 'r', encoding='utf-8') as f:
                    test_data = json.load(f)
                logger.info(f"è¯å…¸æ–‡ä»¶æ ¼å¼æ­£ç¡®ï¼ŒåŒ…å« {len(test_data)} ä¸ªæ¡ç›®")
            except Exception as json_e:
                logger.error(f"è¯å…¸æ–‡ä»¶æ ¼å¼é”™è¯¯: {json_e}", exc_info=True)
    
    @filter.command_group("tangut")
    def tangut(self):
        pass
    
    @tangut.command("gx")
    async def tangut_gx(self, event: AstrMessageEvent):
        """è·å–è¥¿å¤æ–‡çš„é¾šå‹‹æ‹ŸéŸ³"""
        message = event.message_str or ""
        tangut_text = message[len("/tangut gx"):].strip()
        if not tangut_text:
            yield event.plain_result("ç”¨æ³•: /tangut gx <è¥¿å¤æ–‡>")
            return
        
        try:
            if not self.dictionary:
                yield event.plain_result("è¯å…¸æœªåŠ è½½æˆåŠŸï¼Œæ— æ³•è·å–æ‹ŸéŸ³")
                return
            
            # ä½¿ç”¨ BilingualDictionary è·å–é¾šå‹‹æ‹ŸéŸ³
            result = self._get_gx_pronunciation(tangut_text)
            yield event.plain_result(f"é¾šå‹‹æ‹ŸéŸ³: {result}")
        except Exception as e:
            logger.error(f"è·å–é¾šå‹‹æ‹ŸéŸ³å¤±è´¥: {e}", exc_info=True)
            yield event.plain_result(f"è·å–æ‹ŸéŸ³å¤±è´¥: {str(e)}")
    
    @tangut.command("ghc")
    async def tangut_ghc(self, event: AstrMessageEvent):
        """è·å–è¥¿å¤æ–‡çš„é¾šç…ŒåŸæ‹ŸéŸ³"""
        message = event.message_str or ""
        tangut_text = message[len("/tangut ghc"):].strip()
        if not tangut_text:
            yield event.plain_result("ç”¨æ³•: /tangut ghc <è¥¿å¤æ–‡>")
            return
        
        try:
            if not self.dictionary:
                yield event.plain_result("è¯å…¸æœªåŠ è½½æˆåŠŸï¼Œæ— æ³•è·å–æ‹ŸéŸ³")
                return
            
            # ä½¿ç”¨ BilingualDictionary è·å–é¾šç…ŒåŸæ‹ŸéŸ³
            result = self._get_ghc_pronunciation(tangut_text)
            yield event.plain_result(f"é¾šç…ŒåŸæ‹ŸéŸ³: {result}")
        except Exception as e:
            logger.error(f"è·å–é¾šç…ŒåŸæ‹ŸéŸ³å¤±è´¥: {e}", exc_info=True)
            yield event.plain_result(f"è·å–æ‹ŸéŸ³å¤±è´¥: {str(e)}")
    
    @tangut.command("t2zh")
    async def tangut_t2zh(self, event: AstrMessageEvent):
        """å°†è¥¿å¤æ–‡ç¿»è¯‘ä¸ºä¸­æ–‡"""
        message = event.message_str or ""
        tangut_text = message[len("/tangut t2zh"):].strip()
        if not tangut_text:
            yield event.plain_result("ç”¨æ³•: /tangut t2zh <è¥¿å¤æ–‡>")
            return
        
        try:
            if not self.dictionary:
                yield event.plain_result("è¯å…¸æœªåŠ è½½æˆåŠŸï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return
            
            # ä½¿ç”¨ BilingualDictionary è·å–é€å­—é‡Šä¹‰
            literal_meanings = self._get_literal_meanings(tangut_text)
        
            # å°†é€å­—é‡Šä¹‰ä¼ é€’ç»™LLMç”Ÿæˆæœ€ç®€ç¿»è¯‘
            llm_prompt = f"æ ¹æ®ä»¥ä¸‹è¥¿å¤æ–‡é€å­—é‡Šä¹‰ï¼Œç”Ÿæˆæœ€ç®€ç¿»è¯‘ï¼š{literal_meanings}"
        
            # è°ƒç”¨LLMè¿›è¡Œç¿»è¯‘
            provider = self.context.get_using_provider()
            if not provider:
                yield event.plain_result("æœªé…ç½®å¯ç”¨çš„è¯­è¨€æ¨¡å‹")
                return
        
            llm_response = await provider.text_chat(
                prompt=llm_prompt,
                session_id=None,
                contexts=[],
                image_urls=[],
                func_tool=None,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘åŠ©æ‰‹ï¼Œæ“…é•¿å°†é€å­—é‡Šä¹‰è½¬æ¢ä¸ºæµç•…çš„ä¸­æ–‡ç¿»è¯‘ã€‚ä»…è¾“å‡ºç®€æ´æ˜äº†çš„ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ é¢å¤–è§£é‡Šã€‚"
            )
        
            if llm_response.role == "assistant":
                translation = llm_response.completion_text
                yield event.plain_result(f"é€å­—è¯é‡Šä¹‰:\n{literal_meanings}\næœ€ç®€ç¿»è¯‘: {translation}")
            else:
                yield event.plain_result("ç¿»è¯‘å¤±è´¥: LLMè¿”å›å¼‚å¸¸")
        except Exception as e:
            logger.error(f"è¥¿å¤æ–‡ç¿»è¯‘å¤±è´¥: {e}", exc_info=True)
            yield event.plain_result(f"ç¿»è¯‘å¤±è´¥: {str(e)}")
    
    @tangut.command("zh2t")
    async def tangut_zh2t(self, event: AstrMessageEvent):
        """å°†ä¸­æ–‡ç¿»è¯‘ä¸ºè¥¿å¤æ–‡ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰"""
        message = event.message_str or ""
        chinese_text = message[len("/tangut zh2t"):].strip()
        if not chinese_text:
            yield event.plain_result("ç”¨æ³•: /tangut zh2t <ä¸­æ–‡>")
            return
    
        try:
            if not self.dictionary:
                yield event.plain_result("è¯å…¸æœªåŠ è½½æˆåŠŸï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return
            
            # æ­¥éª¤1: ä½¿ç”¨LLMå¯¹ä¸­æ–‡è¿›è¡Œé¢„å¤„ç†ï¼ˆåˆ†è¯å’Œè¯­åºè°ƒæ•´ï¼‰
            preprocessed_text = await self._preprocess_chinese(chinese_text)
            
            # æ­¥éª¤2: åˆ†è¯å¤„ç†
            words = preprocessed_text.split()
            
            # æ­¥éª¤3: å¯¹æ¯ä¸ªè¯åªåŒ¹é…ä¸€ä¸ªè¥¿å¤æ–‡å­—ç¬¦/è¯è¯­
            tangut_result = ""
            for word in words:
                # ä¸ºæ¯ä¸ªè¯æŸ¥æ‰¾åŒ¹é…çš„è¥¿å¤æ–‡å­—ç¬¦/è¯è¯­
                char_result = self._find_single_tangut_char(word)
                tangut_result += char_result
            
            # å‘é€è¥¿å¤æ–‡ç»“æœ
            yield event.plain_result(f"è¥¿å¤æ–‡ç»“æœ: {tangut_result}")
            
            # å¦‚æœæœ‰è¥¿å¤æ–‡ç»“æœï¼Œæ¸²æŸ“ä¸ºå›¾ç‰‡å¹¶å‘é€
            if tangut_result and tangut_result != "æœªæ‰¾åˆ°åŒ¹é…çš„è¥¿å¤æ–‡":
                image_path = self._render_tangut_text(tangut_result)
                if image_path and os.path.exists(image_path):
                    yield event.image_result(image_path)
        except Exception as e:
            logger.error(f"ä¸­æ–‡åˆ°è¥¿å¤æ–‡ç¿»è¯‘å¤±è´¥: {e}", exc_info=True)
            yield event.plain_result(f"ç¿»è¯‘å¤±è´¥: {str(e)}")

    def _find_single_tangut_char(self, chinese_char: str) -> str:
        # ç›´æ¥æŸ¥è¯¢
        results = self.dictionary.search_by_text(chinese_char)
        if results:
            if isinstance(results, list):
                # æŒ‰å­—ç¬¦æ•°å‡åºæ’åºï¼Œå–æœ€çŸ­åŒ¹é…
                sorted_results = sorted(results, key=lambda x: len(x.key))
                return sorted_results[0].key
            return results.key if hasattr(results, 'key') else list(results.keys())[0]

        # æ‹†åˆ†å•å­—æŸ¥è¯¢
        tangut_parts = []
        for char in chinese_char:
            # ä¼˜å…ˆæŸ¥è¯¢åŒ…å«è¯¥å­—çš„å•å­—ç¬¦æ¡ç›®
            single_results = self.dictionary.search_contains(char, field='cn')
            if single_results:
                if isinstance(single_results, list):
                    # è¿‡æ»¤å•å­—ç¬¦ç»“æœå¹¶æŒ‰å­—ç¬¦æ•°æ’åº
                    single_char_results = [r for r in single_results if len(r.key) == 1]
                    if single_char_results:
                        tangut_parts.append(sorted(single_char_results, key=lambda x: len(x.key))[0].key)
                        continue
                elif len(single_results.key) == 1:
                    tangut_parts.append(single_results.key)
                    continue
            tangut_parts.append('ğ—¸ ')
        
        return ''.join(tangut_parts) if any(tangut_parts) else 'ğ—¸ '
    
    @tangut.command("render")
    async def tangut_render(self, event: AstrMessageEvent):
        """å°†è¥¿å¤æ–‡æ¸²æŸ“ä¸ºå›¾ç‰‡"""
        message = event.message_str or ""
        tangut_text = message[len("/tangut render"):].strip()
        if not tangut_text:
            yield event.plain_result("ç”¨æ³•: /tangut render <è¥¿å¤æ–‡>")
            return
        
        try:
            # æ¸²æŸ“è¥¿å¤æ–‡ä¸ºå›¾ç‰‡
            image_path = self._render_tangut_text(tangut_text)
            
            if image_path and os.path.exists(image_path):
                yield event.image_result(image_path)
                os.remove(image_path)
            else:
                yield event.plain_result("è¥¿å¤æ–‡æ¸²æŸ“å¤±è´¥")
        except Exception as e:
            logger.error(f"è¥¿å¤æ–‡æ¸²æŸ“å¤±è´¥: {e}", exc_info=True)
            yield event.plain_result(f"æ¸²æŸ“å¤±è´¥: {str(e)}")
    
    async def _preprocess_chinese(self, chinese_text):
        """ä½¿ç”¨LLMé¢„å¤„ç†ä¸­æ–‡æ–‡æœ¬"""
        llm_prompt = f"""
        è¯·å°†ä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œå¹¶è°ƒæ•´ä¸ºè—ç¼…è¯­åºï¼ˆé€šå¸¸æ˜¯å®¾è¯­åœ¨å‰ï¼ŒåŠ¨è¯åœ¨åï¼‰å¹¶å°½å¯èƒ½æŠŠç”¨è¯æ”¹æˆå¸¸ç”¨è¯ï¼Œç”Ÿåƒ»è¯è¯­ç›´æ¥è¿›è¡Œè¿‘ä¹‰è¯æ›¿æ¢ï¼š
        {chinese_text}
        è¾“å‡ºæ ¼å¼ï¼šåˆ†è¯åçš„è¯ç”¨ç©ºæ ¼åˆ†éš”ï¼Œä¸éœ€è¦è§£é‡Šã€‚ä»…ç»™å‡ºæœ€ç»ˆç»“æœï¼Œä¸è¦æ˜¾ç¤ºä»»ä½•ä¸­é—´æ­¥éª¤ã€‚
        """
        # è°ƒç”¨LLMè¿›è¡Œé¢„å¤„ç†
        provider = self.context.get_using_provider()
        if not provider:
            raise Exception("æœªé…ç½®å¯ç”¨çš„è¯­è¨€æ¨¡å‹")

        llm_response = await provider.text_chat(
            prompt=llm_prompt,
            session_id=None,
            contexts=[],
            image_urls=[],
            func_tool=None,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬å¤„ç†å™¨ï¼Œæ“…é•¿ä¸­æ–‡åˆ†è¯å’Œè¯­åºè°ƒæ•´ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¾“å‡ºï¼Œä¸è¦æ·»åŠ é¢å¤–è§£é‡Šã€‚"
        )

        if llm_response.role == "assistant":
            return llm_response.completion_text.strip()
        else:
            raise Exception
    
    def _get_gx_pronunciation(self, tangut_text):
        """è·å–é¾šå‹‹æ‹ŸéŸ³"""
        pronunciations = []
        for char in tangut_text:
            result = self.dictionary.search_by_key(char)
            if result:
                # å¤„ç†ç»“æœå¯èƒ½æ˜¯å•ä¸ªæ¡ç›®æˆ–æ¡ç›®åˆ—è¡¨
                if isinstance(result, list):
                    # å–ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹çš„GXæ‹ŸéŸ³
                    gx = result[0].GX if result[0] and result[0].GX else char
                else:
                    gx = result.GX if result and result.GX else char
                pronunciations.append(gx)
            else:
                pronunciations.append(char)
        return " ".join(pronunciations)
    
    def _get_ghc_pronunciation(self, tangut_text):
        """è·å–é¾šç…ŒåŸæ‹ŸéŸ³"""
        pronunciations = []
        for char in tangut_text:
            result = self.dictionary.search_by_key(char)
            if result:
                # å¤„ç†ç»“æœå¯èƒ½æ˜¯å•ä¸ªæ¡ç›®æˆ–æ¡ç›®åˆ—è¡¨
                if isinstance(result, list):
                    # å–ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹çš„GHCæ‹ŸéŸ³
                    ghc = result[0].GHC if result[0] and result[0].GHC else char
                else:
                    ghc = result.GHC if result and result.GHC else char
                pronunciations.append(ghc)
            else:
                pronunciations.append(char)
        return " ".join(pronunciations)
    
    def _get_literal_meanings(self, tangut_text):
        """è·å–é€å­—é‡Šä¹‰ï¼ˆæ”¹è¿›ç‰ˆï¼Œæ”¯æŒè¯ç»„ä¼˜å…ˆå’Œå­—ç¬¦ç»„åˆè§„åˆ™ï¼‰"""
        # åˆå§‹åŒ–ç»“æœæ•°ç»„
        meanings = []
        i = 0
        text_length = len(tangut_text)
        
        # è¯ç»„ä¼˜å…ˆåŒ¹é…é€»è¾‘
        while i < text_length:
            current_char = tangut_text[i]
            matched = False
            
            # é¦–å…ˆå°è¯•åŒ¹é…è¯ç»„ï¼ˆæœ€é•¿åŒ¹é…ä¼˜å…ˆï¼‰
            # å°è¯•æŸ¥æ‰¾æœ€é•¿å¯èƒ½çš„è¯ç»„ï¼ˆæœ€å¤š5ä¸ªå­—ç¬¦ï¼Œé˜²æ­¢è¿‡åº¦åŒ¹é…ï¼‰
            max_word_length = min(5, text_length - i)
            found_word = None
            
            for word_length in range(max_word_length, 1, -1):
                candidate = tangut_text[i:i+word_length]
                # ä½¿ç”¨search_by_textæŸ¥æ‰¾è¯ç»„
                word_result = self.dictionary.search_by_text(candidate)
                
                if word_result:
                    # å¦‚æœç»“æœæ˜¯åˆ—è¡¨ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å®Œå…¨åŒ¹é…é¡¹
                    if isinstance(word_result, list):
                        for item in word_result:
                            if hasattr(item, 'key') and item.key == candidate:
                                found_word = item
                                break
                    # å¦‚æœç»“æœæ˜¯å­—å…¸ï¼Œç›´æ¥æ£€æŸ¥key
                    elif hasattr(word_result, 'key') and word_result.key == candidate:
                        found_word = word_result
                    
                    if found_word:
                        meaning = found_word.explanationCN if hasattr(found_word, 'explanationCN') and found_word.explanationCN else candidate
                        meanings.append(meaning)
                        i += word_length
                        matched = True
                        break
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°è¯ç»„ï¼Œå¤„ç†å•ä¸ªå­—ç¬¦
            if not matched:
                # è·å–å‰åå­—ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡
                prev_char = tangut_text[i-1] if i > 0 else None
                next_char = tangut_text[i+1] if i < text_length - 1 else None
                
                # æŸ¥è¯¢å•ä¸ªå­—ç¬¦
                result = self.dictionary.search_by_key(current_char)
                
                if result:
                    # å¤„ç†ç»“æœå¯èƒ½æ˜¯å•ä¸ªæ¡ç›®æˆ–æ¡ç›®åˆ—è¡¨
                    if isinstance(result, list):
                        # å–ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹çš„ä¸­æ–‡è§£é‡Š
                        meaning = result[0].explanationCN if result[0] and hasattr(result[0], 'explanationCN') and result[0].explanationCN else current_char
                    else:
                        meaning = result.explanationCN if hasattr(result, 'explanationCN') and result.explanationCN else current_char
                    meanings.append(meaning)
                else:
                    meanings.append(current_char)
                
                i += 1
        
        return "    ".join(meanings)
    
    def _find_tangut_by_chinese(self, chinese_text):
        """æ ¹æ®ä¸­æ–‡æ–‡æœ¬æŸ¥æ‰¾è¥¿å¤æ–‡"""
        # ä½¿ç”¨search_by_textè¿›è¡Œåå‘æŸ¥è¯¢
        results = self.dictionary.search_by_text(chinese_text, lang="cn")
        
        if not results:
            return "æœªæ‰¾åˆ°åŒ¹é…çš„è¥¿å¤æ–‡"
        
        # æå–è¥¿å¤æ–‡å­—ç¬¦
        tangut_chars = []
        for entry in results[:10]:  # é™åˆ¶è¿”å›ç»“æœæ•°é‡
            if hasattr(entry, 'key'):
                tangut_chars.append(entry.key)
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        unique_chars = [char for char in tangut_chars if not (char in seen or seen.add(char))]
        
        return "".join(unique_chars[:20]) 
    
    def _render_tangut_text(self, tangut_text):
        """å°†è¥¿å¤æ–‡æ–‡æœ¬æ¸²æŸ“ä¸ºå›¾ç‰‡"""
        try:
            font_size = 60
            
            if not os.path.exists(self.font_path):
                logger.error(f"å­—ä½“æ–‡ä»¶ä¸å­˜åœ¨: {self.font_path}")
                return None
            
            font = ImageFont.truetype(self.font_path, font_size)

            bbox = font.getbbox(tangut_text)
            text_width = bbox[2] - bbox[0]  # right - left
            text_height = bbox[3] - bbox[1]  # bottom - top

            padding = 20
            image_width = text_width + padding * 2
            image_height = text_height + padding * 2
            
            image = Image.new('RGBA', (image_width, image_height), (255, 255, 255, 255))
            draw = ImageDraw.Draw(image)
    
            draw.text((padding, padding), tangut_text, font=font, fill=(0, 0, 0, 255))
            
            temp_dir = os.path.join(self.plugin_dir, "temp")
            if not os.path.exists(temp_dir):
                os.makedirs(temp_dir)
            
            image_path = os.path.join(temp_dir, f"tangut_render_{hash(tangut_text)}.png")
            image.save(image_path, "PNG")
            
            return image_path
        except Exception as e:
            logger.error(f"æ¸²æŸ“è¥¿å¤æ–‡å¤±è´¥: {e}", exc_info=True)
            return None